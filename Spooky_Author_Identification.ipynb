{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spooky_Author_Identification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Damntoochill/Learning-ML/blob/master/Spooky_Author_Identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "U8r8VuDZR0dV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Objective : \n",
        "\n",
        "   The  dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley. The data was prepared by chunking larger texts into sentences using CoreNLP's MaxEnt sentence tokenizer. The objective is to accurately identify the author of the sentences in the test set."
      ]
    },
    {
      "metadata": {
        "id": "Y4H1hloep1Mc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Uploading the dataset\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N0VsWbhqS6OC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Importing the required dependencies\n",
        "import nltk\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vUl1lOqTwzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "043a7d05-4709-4e71-f2cf-786e3ab5823c"
      },
      "cell_type": "code",
      "source": [
        "#Creating a Dataframe\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id26305</td>\n",
              "      <td>This process, however, afforded me no means of...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id17569</td>\n",
              "      <td>It never once occurred to me that the fumbling...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id11008</td>\n",
              "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27763</td>\n",
              "      <td>How lovely is spring As we looked from Windsor...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id12958</td>\n",
              "      <td>Finding nothing else, not even gold, the Super...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text author\n",
              "0  id26305  This process, however, afforded me no means of...    EAP\n",
              "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
              "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
              "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
              "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "vbj2-UwvUxxg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To find out which author wrote a sentence we need to find the frequency of words used by each author. For that, first we need to split the data by each author. Then we can find the frequency of words in each of these data groups that belong to each author."
      ]
    },
    {
      "metadata": {
        "id": "M0KHBaaqT2_-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Splitting the data by author\n",
        "\n",
        "data_groups = data.groupby('author')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zV8swvCSYOXO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We need to define a function to create dictionary of  frequencies for each words from the grouped data for each author. Luckily the nltk library has the function built in."
      ]
    },
    {
      "metadata": {
        "id": "9k5oowPvW3DN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Function to find the frequency of words\n",
        "\n",
        "word_frequency = nltk.probability.ConditionalFreqDist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ujPv883IYha3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before we apply the function we need to combine the sentences(the text column) told by each author. Then we need to convert all the words to lower case (to eliminate the ambiguity between words with upper and lower case letters). Then we need to tokenize each word and calculate the frequency of each words. we would add the frequency of each word to a dictionary."
      ]
    },
    {
      "metadata": {
        "id": "4ms78FjRee3W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "739e9a4a-4880-42bc-a13a-35cc104d1bd1"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "Wx8nG9LfXYEk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Finding the fequency of words used by each author\n",
        "\n",
        "for name, group in data_groups :\n",
        "  sentences = group['text'].str.cat(sep = ' ') #Combines all the sentences\n",
        "  sentences = sentences.lower() #Converts all sentences to lower case\n",
        "  tokens = nltk.tokenize.word_tokenize(sentences) #Tokenize the combined sentence\n",
        "  frequency = nltk.FreqDist(tokens) #Calculates the frequency of each token and returns a dictionary\n",
        "  word_frequency[name] = (frequency) #Returns a dictionary of dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sNgmRwbJerTU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now lets check how often an author uses a word."
      ]
    },
    {
      "metadata": {
        "id": "cLOZy-owenmd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "ccee9621-5044-4c3b-f4e7-8e8ad94eaa9c"
      },
      "cell_type": "code",
      "source": [
        "for i in word_frequency.keys():\n",
        "  print(\"haunt : \" + i)\n",
        "  print(word_frequency[i].freq('haunt'))\n",
        "  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "haunt : EAP\n",
            "4.3077638828460535e-06\n",
            "haunt : HPL\n",
            "4.5984675606854016e-05\n",
            "haunt : MWS\n",
            "1.5888147442008263e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r2awcPL7fVdN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now lets use the dictionary of dictionary we generated above to determine which author wrote  a sentence. For that first we need to create a data frame to contain the dictionary we generated followed by tokenizing the sentence for which we need to find the author. Then we have to find the probability/frequency of each token ( present in the sentence ) in the dataframe grouped by the author names and add them together. The group with maximum probabiity determines the author of the given sentence."
      ]
    },
    {
      "metadata": {
        "id": "02XBYzxTfNol",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predictor(test_sentence):\n",
        "  \n",
        "  processed_test_sentence = nltk.tokenize.word_tokenize(test_sentence.lower())\n",
        "  \n",
        "  test_probabilities = pd.DataFrame(columns = ['author', 'word', 'probability'])\n",
        "\n",
        "  for i in word_frequency.keys():\n",
        "    for j in processed_test_sentence:\n",
        "      token_freq = word_frequency[i].freq(j)\n",
        "      smoothed_token_freq = token_freq + 0.000001\n",
        "      output = pd.DataFrame([[i,j,token_freq]], columns = ['author','word','probability'])\n",
        "      test_probabilities = test_probabilities.append(output, ignore_index = True)\n",
        "  total_probabilities = []\n",
        "  #total_probabilities = pd.DataFrame(columns = ['author', 'total_probability'])\n",
        "  for i in word_frequency.keys():\n",
        "    single_author = test_probabilities.query('author == \"' + i + '\"')\n",
        "    total_probability = single_author.product(numeric_only = True)[0]\n",
        "    total_probabilities.append(total_probability)\n",
        "    #output = pd.DataFrame([[i, total_probability]], columns = ['author','total_probability'])\n",
        "    #total_probabilities = total_probabilities.append(output,ignore_index = True)\n",
        "  return total_probabilities\n",
        "  #return total_probabilities.loc[total_probabilities['total_probability'].idxmax(),'author']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rrzTDyN1X8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d8d20181-58d8-43ea-f655-d0bcbd7f4bff"
      },
      "cell_type": "code",
      "source": [
        "sentence = \"the sky was ghastly blue\"\n",
        "probability = predictor(sentence)\n",
        "print(probability)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.648284698631943e-16, 6.1946985157252966e-15, 4.538885719571753e-16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LBZoWNILqC0G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "f1277230-59e3-4133-e23b-aca9928662ea"
      },
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('test.csv')\n",
        "test_data.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id02310</td>\n",
              "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id24541</td>\n",
              "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id00134</td>\n",
              "      <td>And when they had broken down the frail door t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27757</td>\n",
              "      <td>While I was thinking how I should possibly man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id04081</td>\n",
              "      <td>I am not sure to what limit his knowledge may ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text\n",
              "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
              "1  id24541  If a fire wanted fanning, it could readily be ...\n",
              "2  id00134  And when they had broken down the frail door t...\n",
              "3  id27757  While I was thinking how I should possibly man...\n",
              "4  id04081  I am not sure to what limit his knowledge may ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "JX6dmaBbzjp_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}